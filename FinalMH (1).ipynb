{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "b6tm-d3cbe9m"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path='/content/training.csv'"
      ],
      "metadata": {
        "id": "pBHpQV42blMu"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(file_path):\n",
        "    dataset = pd.read_csv(file_path)\n",
        "    print(\"Dataset loaded. First few rows:\")\n",
        "    print(dataset.head())\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "tA5jNl80e_KO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(dataset):\n",
        "    texts = dataset['text'].tolist()\n",
        "    # Encode emotions: positive as 1, neutral as 0, and negative as -1\n",
        "    labels = dataset['label'].tolist()\n",
        "\n",
        "    # Check dataset structure\n",
        "    print(f\"Sample texts: {texts[:2]}\")\n",
        "    print(f\"Sample labels: {labels[:2]}\")\n",
        "\n",
        "    return texts, labels"
      ],
      "metadata": {
        "id": "-MpgOhQjfETG"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_mapping = {\n",
        "    'joy': 'positive',\n",
        "    'surprise': 'positive',\n",
        "    'trust': 'positive',\n",
        "    'love': 'positive',\n",
        "    'neutral': 'neutral',\n",
        "\n",
        "    'anger': 'negative',\n",
        "    'fear': 'negative',\n",
        "    'sadness': 'negative',\n",
        "    'disgust': 'negative'\n",
        "}\n",
        "\n",
        "def map_emotion_to_polarity(emotion):\n",
        "    return emotion_mapping.get(emotion, 'neutral')"
      ],
      "metadata": {
        "id": "3YfjkGmafE3p"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_emotion_model():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"j-hartmann/emotion-english-distilroberta-base\")\n",
        "    emotion_model = pipeline('text-classification', model='j-hartmann/emotion-english-distilroberta-base', return_all_scores=True)\n",
        "    return emotion_model"
      ],
      "metadata": {
        "id": "BaZs2Tn_fY5c"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_emotions_polarity(emotion_model, tweets):\n",
        "    emotions = emotion_model(tweets)\n",
        "\n",
        "    # Convert predicted emotions to the defined labels\n",
        "    polarities = []\n",
        "    for tweet_emotions in emotions:\n",
        "        # Select the emotion with the highest score\n",
        "        main_emotion = max(tweet_emotions, key=lambda x: x['score'])['label']\n",
        "        polarity = map_emotion_to_polarity(main_emotion)\n",
        "        polarities.append(polarity)  # Append the polarity directly (1, 0, -1)\n",
        "\n",
        "    return polarities"
      ],
      "metadata": {
        "id": "VXj8KwpvfbOK"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(true_labels, predicted_labels):\n",
        "    # Filter out -1 labels for evaluation\n",
        "    filtered_labels = [(true, pred) for true, pred in zip(true_labels, predicted_labels) if true != -1]\n",
        "\n",
        "\n",
        "\n",
        "    true_labels_filtered, predicted_labels_filtered = zip(*filtered_labels)\n",
        "\n",
        "    accuracy = accuracy_score(true_labels_filtered, predicted_labels_filtered)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels_filtered, predicted_labels_filtered, average='weighted')  # Use weighted for multi-class\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    return accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "h5axdQC9fbur"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_dataset(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqyVTJO3CO2w",
        "outputId": "da73e6ac-3c85-48cb-d9d2-8ba0474813c5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded. First few rows:\n",
            "                                                text  label  Unnamed: 2  \\\n",
            "0                            i didnt feel humiliated     10         NaN   \n",
            "1  i can go from feeling so hopeless to so damned...     10         NaN   \n",
            "2   im grabbing a minute to post i feel greedy wrong     20         NaN   \n",
            "3  i am ever feeling nostalgic about the fireplac...     10         NaN   \n",
            "4                               i am feeling grouchy     20         NaN   \n",
            "\n",
            "   Unnamed: 3  Unnamed: 4  Unnamed: 5   Unnamed: 6 Unnamed: 7  \n",
            "0         NaN         NaN         NaN          NaN        NaN  \n",
            "1         NaN         NaN         NaN          NaN        NaN  \n",
            "2         NaN         NaN         NaN           10         10  \n",
            "3         NaN         NaN         NaN  10,10,10,10     20,20,  \n",
            "4         NaN         NaN         NaN          NaN        NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_model = load_emotion_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48r2nI04Cyea",
        "outputId": "9c41ed9d-fef4-4e39-80a7-cfad3cbcfda9"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts,label=preprocess_dataset(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO6b2buxC6gu",
        "outputId": "2a648042-965c-4950-f6ba-1b392492423a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample texts: ['i didnt feel humiliated', 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake']\n",
            "Sample labels: [10, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_samples(texts, labels, sample_size=1000):\n",
        "    # Zip the texts and labels together for consistent pairing\n",
        "    combined = list(zip(texts, labels))\n",
        "\n",
        "    # Randomly sample 1000 pairs of texts and labels\n",
        "    sampled_combined = random.sample(combined, sample_size)\n",
        "\n",
        "    # Unzip the sampled texts and labels back into separate lists\n",
        "    sampled_texts, sampled_labels = zip(*sampled_combined)\n",
        "\n",
        "    return list(sampled_texts), list(sampled_labels)"
      ],
      "metadata": {
        "id": "5AnI2dPrDl1G"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "sampled_texts, sampled_labels = get_random_samples(texts, label, sample_size=1000)\n"
      ],
      "metadata": {
        "id": "rmHIliz_IoJS"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = analyze_emotions_polarity(emotion_model, sampled_texts)"
      ],
      "metadata": {
        "id": "34JWo4bxD5qm"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = [10 if label == 'positive' else 20 for label in pred]"
      ],
      "metadata": {
        "id": "Jlq2O6_dOO2a"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "def evaluate_predictions(pred, actual):\n",
        "    # Ensure the lists are the same length\n",
        "    if len(pred) != len(actual):\n",
        "        print(\"Error: Lengths of predicted and actual labels do not match.\")\n",
        "        return\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(actual, pred)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score (for multi-class classification)\n",
        "    precision = precision_score(actual, pred, average='weighted')  # 'weighted' accounts for class imbalance\n",
        "    recall = recall_score(actual, pred, average='weighted')\n",
        "    f1 = f1_score(actual, pred, average='weighted')\n",
        "\n",
        "    # Generate a classification report\n",
        "    report = classification_report(actual, pred)\n",
        "\n",
        "    # Print the metrics\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(report)"
      ],
      "metadata": {
        "id": "fE5Wc2v2LaJ5"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_predictions(pred, sampled_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmKcjRUwNEtr",
        "outputId": "ef527d5b-009b-4365-af4a-1d013952d34a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6570\n",
            "Precision: 0.8403\n",
            "Recall: 0.6570\n",
            "F1 Score: 0.6795\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          10       0.98      0.56      0.71       759\n",
            "          20       0.41      0.96      0.57       241\n",
            "\n",
            "    accuracy                           0.66      1000\n",
            "   macro avg       0.69      0.76      0.64      1000\n",
            "weighted avg       0.84      0.66      0.68      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fGqmPqY8OfrY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}